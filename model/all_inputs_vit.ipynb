{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"},{"sourceId":8108731,"sourceType":"datasetVersion","datasetId":4468693},{"sourceId":8150999,"sourceType":"datasetVersion","datasetId":4820725}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* This notebook is a modified version of https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as  imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\n\nimport torch\nimport timm\nimport glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\nimport math\nimport warnings\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:12.741724Z","iopub.execute_input":"2024-04-18T02:05:12.742106Z","iopub.status.idle":"2024-04-18T02:05:12.750850Z","shell.execute_reply.started":"2024-04-18T02:05:12.742073Z","shell.execute_reply":"2024-04-18T02:05:12.749683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE0 = 512\n    IMAGE_SIZE = 224\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    TARGET_COLUMNS_TEST = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n    N_TARGETS = len(TARGET_COLUMNS)\n    # Dataset\n    RECOMPUTE_DATAFRAMES = False\n    BATCH_SIZE = 96\n    BATCH_SIZE_VAL = 128\n    N_VAL_SAMPLES0 = 4096\n    # Training\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 8\n    TRAIN_MODEL = True\n    # Others\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n    SEED = 42\n    EPS = 1e-6\n    EPS_CUDA = torch.tensor([EPS]).to('cuda')\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:12.753227Z","iopub.execute_input":"2024-04-18T02:05:12.753701Z","iopub.status.idle":"2024-04-18T02:05:12.764241Z","shell.execute_reply.started":"2024-04-18T02:05:12.753667Z","shell.execute_reply":"2024-04-18T02:05:12.763416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train DataFrame","metadata":{}},{"cell_type":"code","source":"if CONFIG.RECOMPUTE_DATAFRAMES:\n    train0 = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n\n    # Add File Path\n    train0['file_path'] = train0['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n\n    # Reaed Raw Image JPEG Bytes\n    train0['jpeg_bytes'] = train0['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    \n    # Save for Future Use\n    train0.to_pickle('train.pkl')\nelse:\n    train0 = pd.read_pickle('/kaggle/input/planttraits2024-eda-training-pub-dataset/train.pkl')\n    \n# Assign Medians\nCONFIG.TARGET_MEDIANS = train0[CONFIG.TARGET_COLUMNS].median(axis=0).values","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:12.765334Z","iopub.execute_input":"2024-04-18T02:05:12.765694Z","iopub.status.idle":"2024-04-18T02:05:15.435073Z","shell.execute_reply.started":"2024-04-18T02:05:12.765670Z","shell.execute_reply":"2024-04-18T02:05:15.434173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train in train/val\ntrain, val = train_test_split(train0, test_size=0.2, shuffle=True, random_state=CONFIG.SEED)\n\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:15.436239Z","iopub.execute_input":"2024-04-18T02:05:15.436581Z","iopub.status.idle":"2024-04-18T02:05:15.568750Z","shell.execute_reply.started":"2024-04-18T02:05:15.436556Z","shell.execute_reply":"2024-04-18T02:05:15.567682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test DataFrame","metadata":{}},{"cell_type":"code","source":"if CONFIG.RECOMPUTE_DATAFRAMES:\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n\n    # Add File Path\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n\n    # Reaed Raw Image JPEG Bytes\n    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n\n    # Save for Future Use\n    test.to_pickle('test.pkl')\nelse:\n    test = pd.read_pickle('/kaggle/input/test-data/test.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:15.571726Z","iopub.execute_input":"2024-04-18T02:05:15.572108Z","iopub.status.idle":"2024-04-18T02:05:16.091271Z","shell.execute_reply.started":"2024-04-18T02:05:15.572075Z","shell.execute_reply":"2024-04-18T02:05:16.090435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Columns\nFEATURE_COLUMNS = test.columns.values[1:-2]\nCONFIG.N_FEATURES = len(FEATURE_COLUMNS)\nprint(f'N_FEATURES: {CONFIG.N_FEATURES}')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.093055Z","iopub.execute_input":"2024-04-18T02:05:16.093469Z","iopub.status.idle":"2024-04-18T02:05:16.099434Z","shell.execute_reply.started":"2024-04-18T02:05:16.093433Z","shell.execute_reply":"2024-04-18T02:05:16.098583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Minimum/Maximum Based On Train 0.1% and 99.9%\nCONFIG.V_MIN = train[CONFIG.TARGET_COLUMNS].quantile(0.001)\nCONFIG.V_MAX = train[CONFIG.TARGET_COLUMNS].quantile(0.999)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.100506Z","iopub.execute_input":"2024-04-18T02:05:16.100806Z","iopub.status.idle":"2024-04-18T02:05:16.122759Z","shell.execute_reply.started":"2024-04-18T02:05:16.100782Z","shell.execute_reply":"2024-04-18T02:05:16.121975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labels","metadata":{}},{"cell_type":"code","source":"# Labels Meta Data\ntarget_name_meta = pd.read_csv('/kaggle/input/planttraits2024/target_name_meta.tsv', delimiter='\\t')\ntarget_name_meta['trait_ID'] = target_name_meta['trait_ID'] + '_mean'\ntarget_name_meta = target_name_meta.set_index('trait_ID').squeeze().to_dict()\n\ndisplay(pd.Series(target_name_meta).to_frame())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.124095Z","iopub.execute_input":"2024-04-18T02:05:16.124415Z","iopub.status.idle":"2024-04-18T02:05:16.137953Z","shell.execute_reply.started":"2024-04-18T02:05:16.124390Z","shell.execute_reply":"2024-04-18T02:05:16.136905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentiles of features to use\npercentiles = [\n    0.001, 0.01,0.05,0.10,0.25,\n    0.50,\n    0.75,0.90,0.95,0.99, 0.999,\n]\nlabels_describe_df = pd.DataFrame()\nfor target in CONFIG.TARGET_COLUMNS:\n    labels_describe_df = pd.concat((\n        labels_describe_df,\n        train[target].describe(percentiles=percentiles).round(3)\n    ), axis=1)\n    \n# Transpose DataFrame\nlabels_describe_df = labels_describe_df.T\n    \n# Minimum/Maximum Values\nlabels_describe_df.insert(4, 'v_min', CONFIG.V_MIN)\nlabels_describe_df.insert(16, 'v_max', CONFIG.V_MAX)\n    \ndisplay(labels_describe_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.139128Z","iopub.execute_input":"2024-04-18T02:05:16.139440Z","iopub.status.idle":"2024-04-18T02:05:16.200803Z","shell.execute_reply.started":"2024-04-18T02:05:16.139405Z","shell.execute_reply":"2024-04-18T02:05:16.199840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot outliers","metadata":{}},{"cell_type":"markdown","source":"# Filter Outliers","metadata":{}},{"cell_type":"code","source":"# Mask to exclude values outside of 0.1% - 99.9% range\ndef get_mask(df):\n    lower = []\n    higher = []\n    mask = np.empty(shape=df[CONFIG.TARGET_COLUMNS].shape, dtype=bool)\n    # Fill mask based on minimum/maximum values of sample submission\n    for idx, (t, v_min, v_max) in enumerate(zip(CONFIG.TARGET_COLUMNS, CONFIG.V_MIN, CONFIG.V_MAX)):\n        labels = df[t].values\n        mask[:,idx] = ((labels > v_min) & (labels < v_max))\n    return mask.min(axis=1)\n\n# Masks\nCONFIG.MASK_TRAIN = get_mask(train)\nCONFIG.MASK_VAL = get_mask(val)\n# Masked DataFrames\ntrain_mask = train[CONFIG.MASK_TRAIN].reset_index(drop=True)\nval_mask = val[CONFIG.MASK_VAL].reset_index(drop=True)\n# Add Number Of Steps\nCONFIG.N_TRAIN_SAMPLES = len(train_mask)\nCONFIG.N_VAL_SAMPLES = len(val_mask)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_VAL_STEPS_PER_EPOCH = math.ceil(CONFIG.N_VAL_SAMPLES / CONFIG.BATCH_SIZE_VAL)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\nfor m, subset in zip([CONFIG.MASK_TRAIN, CONFIG.MASK_VAL], ['train', 'val']):\n    print(f'===== {subset} shape: {m.shape} =====')\n    print(f'{subset} \\t| # Masked Samples: {(1-m.mean())*CONFIG.N_TRAIN_SAMPLES:.0f}')\n    print(f'{subset} \\t| % Masked Samples: {100-m.mean()*100:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.202087Z","iopub.execute_input":"2024-04-18T02:05:16.202392Z","iopub.status.idle":"2024-04-18T02:05:16.289208Z","shell.execute_reply.started":"2024-04-18T02:05:16.202367Z","shell.execute_reply":"2024-04-18T02:05:16.288304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Normalization","metadata":{}},{"cell_type":"code","source":"# Log Scale Features\nLOG_FEATURES = ['X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.290397Z","iopub.execute_input":"2024-04-18T02:05:16.290692Z","iopub.status.idle":"2024-04-18T02:05:16.294851Z","shell.execute_reply.started":"2024-04-18T02:05:16.290668Z","shell.execute_reply":"2024-04-18T02:05:16.293985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill labels using normalization tool\ndef fill_y(y, df, normalize=False):\n    for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n        v = df[target]\n        if normalize:\n            # Log10 Transform\n            if target in LOG_FEATURES:\n                v = np.log10(v)\n            # Shift To Have Zero Median\n            Y_SHIFT[target_idx] = np.mean(v)\n            v = v - np.median(v)\n            # Uniform Variance\n            Y_STD[target_idx] = np.std(v)\n            v = v / np.std(v)\n        # Assign to y_train\n        y[:,target_idx] = v\n\n# Feature Scaler\nY_SHIFT = np.zeros(CONFIG.N_TARGETS)\nY_STD = np.zeros(CONFIG.N_TARGETS)\n# Masked Labels\ny_train_mask_raw = np.zeros_like(train_mask[CONFIG.TARGET_COLUMNS], dtype=np.float32)\ny_train_mask = np.zeros_like(train_mask[CONFIG.TARGET_COLUMNS], dtype=np.float32)\ny_val_mask = np.zeros_like(val_mask[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n# Fill Target Arrays\nfill_y(y_train_mask_raw, train_mask, normalize=False)\nfill_y(y_train_mask, train_mask, normalize=True)\nfill_y(y_val_mask, val_mask, normalize=True)\n# Values\ndisplay(pd.DataFrame({\n    'y_shift': Y_SHIFT,\n    'y_std': Y_STD\n}, index=CONFIG.TARGET_COLUMNS))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.296007Z","iopub.execute_input":"2024-04-18T02:05:16.296293Z","iopub.status.idle":"2024-04-18T02:05:16.342633Z","shell.execute_reply.started":"2024-04-18T02:05:16.296246Z","shell.execute_reply":"2024-04-18T02:05:16.341648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Features","metadata":{}},{"cell_type":"code","source":"# Standard Scaler for Features\nFEATURE_SCALER = StandardScaler()\n\n# Fit and transform on training features\ntrain_features_mask = FEATURE_SCALER.fit_transform(train_mask[FEATURE_COLUMNS].values.astype(np.float32))\n# Transform val/test features using scaler fitted on train data\nval_features_mask = FEATURE_SCALER.transform(val_mask[FEATURE_COLUMNS].values.astype(np.float32))\ntest_features = FEATURE_SCALER.transform(test[FEATURE_COLUMNS].values.astype(np.float32))\n# Convert Features to Torch Tensors\ntrain_features_mask = torch.tensor(train_features_mask)\nval_features_mask = torch.tensor(val_features_mask)\ntest_features = torch.tensor(test_features)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.343978Z","iopub.execute_input":"2024-04-18T02:05:16.344377Z","iopub.status.idle":"2024-04-18T02:05:16.536679Z","shell.execute_reply.started":"2024-04-18T02:05:16.344342Z","shell.execute_reply":"2024-04-18T02:05:16.535708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforms","metadata":{}},{"cell_type":"code","source":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n# Training Augmentations\nTRAIN_TRANSFORMS = A.Compose([\n        A.RandomSizedCrop(\n            [int(0.85*CONFIG.IMAGE_SIZE0), CONFIG.IMAGE_SIZE0],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=1.0\n        ),\n        A.HorizontalFlip(p=0.50),\n        A.RandomBrightnessContrast(brightness_limit=0.10, contrast_limit=0.10, p=0.50),\n        A.ImageCompression(quality_lower=75, quality_upper=100, p=0.5),\n        ToTensorV2(),\n    ])\n# Test Augmentations\nVAL_TEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE,CONFIG.IMAGE_SIZE),\n        ToTensorV2(),\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.541730Z","iopub.execute_input":"2024-04-18T02:05:16.542045Z","iopub.status.idle":"2024-04-18T02:05:16.549597Z","shell.execute_reply.started":"2024-04-18T02:05:16.542020Z","shell.execute_reply":"2024-04-18T02:05:16.548555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, X_jpeg_bytes, y, features, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.y = y\n        self.features = features\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        X_sample = {\n            'image': self.transforms(\n                    image=imageio.imread(self.X_jpeg_bytes[index]),\n                )['image'],\n            'feature': self.features[index],\n        }\n        y_sample = self.y[index]\n            \n        return X_sample, y_sample","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.550763Z","iopub.execute_input":"2024-04-18T02:05:16.551108Z","iopub.status.idle":"2024-04-18T02:05:16.563868Z","shell.execute_reply.started":"2024-04-18T02:05:16.551077Z","shell.execute_reply":"2024-04-18T02:05:16.563036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\ntrain_dataset = MyDataset(\n    train_mask['jpeg_bytes'].values,\n    y_train_mask,\n    train_features_mask,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n    )\ntrain_dataloader_iter = iter(train_dataloader)\n\n# Validation\nval_dataset = MyDataset(\n    val_mask['jpeg_bytes'].values,\n    y_val_mask,\n    val_features_mask,\n    VAL_TEST_TRANSFORMS,\n)\nval_dataloader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE_VAL, drop_last=False)\n\n# Test\ntest_dataset = MyDataset(\n    test['jpeg_bytes'].values,\n    test['id'].values,\n    test_features,\n    VAL_TEST_TRANSFORMS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:16.565196Z","iopub.execute_input":"2024-04-18T02:05:16.565835Z","iopub.status.idle":"2024-04-18T02:05:16.983845Z","shell.execute_reply.started":"2024-04-18T02:05:16.565802Z","shell.execute_reply":"2024-04-18T02:05:16.973724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Benchmark Dataset\nN = 1\nt_start = time.perf_counter_ns()\nfor _ in tqdm(range(N)):\n    next(train_dataloader_iter)\nn_images_per_second = (N * CONFIG.BATCH_SIZE) / (time.perf_counter_ns() - t_start) * 1e9\nprint(f'# Images/Second: {n_images_per_second:.0f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:17.014500Z","iopub.execute_input":"2024-04-18T02:05:17.015166Z","iopub.status.idle":"2024-04-18T02:05:17.681215Z","shell.execute_reply.started":"2024-04-18T02:05:17.015087Z","shell.execute_reply":"2024-04-18T02:05:17.680090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example batch\nX_batch, y_batch = next(train_dataloader_iter)\nfor k, v in X_batch.items():\n    print(f'X_batch {k} shape: {v.shape}, dtype: {v.dtype}')\n    print(f'X_batch {k} min: {v.min():.3f}, max: {v.max():.3f}')\n    print(f'X_batch {k} µ: {v.float().mean():.3f}, σ: {v.float().std():.3f}')\n# Label\nprint(f'y_batch shape: {y_batch.shape}, dtype: {y_batch.dtype}')\nprint(f'y_batch min: {y_batch.min():.3f}, max: {y_batch.max():.3f}')\nprint(f'y_batch µ: {y_batch.mean():.3f}, σ: {y_batch.std():.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:17.682839Z","iopub.execute_input":"2024-04-18T02:05:17.691553Z","iopub.status.idle":"2024-04-18T02:05:17.873186Z","shell.execute_reply.started":"2024-04-18T02:05:17.691507Z","shell.execute_reply":"2024-04-18T02:05:17.872149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Count model parameters\ndef count_parameters(model):\n    return sum([p.numel() for p in model.parameters()])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:17.878990Z","iopub.execute_input":"2024-04-18T02:05:17.882786Z","iopub.status.idle":"2024-04-18T02:05:17.897021Z","shell.execute_reply.started":"2024-04-18T02:05:17.882746Z","shell.execute_reply":"2024-04-18T02:05:17.894105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # ImageNet Normalize Input\n        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        # Backbone\n        self.backbone = timm.create_model(\n                'vit_base_patch16_224.augreg_in21k',\n                pretrained=True,\n                num_classes=0,\n            )\n        \n        # Features+Images' representation\n        self.features = nn.Sequential(\n            nn.Linear(768+CONFIG.N_FEATURES,512),\n            nn.GELU(),\n            nn.Linear(512,256),\n            nn.GELU(),\n            nn.Linear(256,256),\n        )\n        \n        # Label\n        self.label = nn.Sequential(\n            nn.Linear(256,256),\n            nn.GELU(),\n            nn.Linear(256,CONFIG.N_TARGETS, bias=False),\n        )\n        \n        # Initialize Weights\n        self.initialize_weights()\n        \n    def initialize_weights(self):\n        # Features\n        nn.init.kaiming_uniform_(self.features[2].weight)\n        # Label\n        nn.init.zeros_(self.label[2].weight)\n        \n    def forward(self, inputs, debug=False):\n        cated_features = torch.cat((self.backbone(self.normalize(inputs['image'].float() / 255)),inputs['feature']),dim=1)\n        return {\n            'label': self.label(\n                 self.features(cated_features)\n            )\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:17.899206Z","iopub.execute_input":"2024-04-18T02:05:17.899594Z","iopub.status.idle":"2024-04-18T02:05:17.938333Z","shell.execute_reply.started":"2024-04-18T02:05:17.899561Z","shell.execute_reply":"2024-04-18T02:05:17.936985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# from torchview import draw_graph\n# model_graph = draw_graph(model.features, input_size=(1,931),device=\"cuda\")\n# model_graph.visual_graph\n# Clear torch cache\ntorch.cuda.empty_cache()\n\n# Load Weights if model is not trained\nif not CONFIG.TRAIN_MODEL:\n    model = torch.load('/kaggle/input/planttraits2024-eda-dataset/model.pth')\nelse:\n    # Create new Model\n    model = Model()\n\n# Model to GPU memory\nmodel = model.to('cuda')\n\n\n\nprint(f'# Model Parameters: {count_parameters(model):,}')\n\nwith torch.no_grad():\n    # Put inputs on GPU\n    for k, v in X_batch.items():\n        X_batch[k] = v.to('cuda')\n    outputs = model(X_batch, debug=True)\n    for k, v in outputs.items():\n        print(f'outputs {k} shape: {v.shape}, min: {v.min():.3f}, max: {v.max():.3f}, µ: {v.mean():.3f}, σ: {v.std():.3f}')\n    # Label Outputs\n    for o in outputs['label'][:3,:]:\n        print(o.detach().cpu().numpy().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:17.944977Z","iopub.execute_input":"2024-04-18T02:05:17.948577Z","iopub.status.idle":"2024-04-18T02:05:21.008400Z","shell.execute_reply.started":"2024-04-18T02:05:17.948537Z","shell.execute_reply":"2024-04-18T02:05:21.007461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Rate Schedule","metadata":{}},{"cell_type":"code","source":"# Get the learning rate scheduler\ndef get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.10,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.009715Z","iopub.execute_input":"2024-04-18T02:05:21.009997Z","iopub.status.idle":"2024-04-18T02:05:21.015207Z","shell.execute_reply.started":"2024-04-18T02:05:21.009975Z","shell.execute_reply":"2024-04-18T02:05:21.014245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Learning Rate Scheduler\ndef plot_lr_scheduler():\n    lr_scheduler = get_lr_scheduler(torch.optim.Adam(model.parameters()))\n    lrs  = []\n    for step in range(CONFIG.N_STEPS):\n        lrs.append(lr_scheduler.get_last_lr())\n        lr_scheduler.step()\n    # Plot Learning Rate\n    plt.figure(figsize=(12,5))\n    plt.title('Learning Rate Schedule')\n    plt.xlim(0, CONFIG.N_STEPS)\n    plt.ylim(0, CONFIG.LR_MAX*1.1)\n    plt.xlabel('Step')\n    plt.ylabel('Learning Rate')\n    plt.plot(lrs)\n    plt.grid()\n    plt.show()\n    # Reset Learning Rate Scheduler\n    lr_scheduler._step_count = 0\n    lr_scheduler.last_epoch = 0\n\nplot_lr_scheduler()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.016466Z","iopub.execute_input":"2024-04-18T02:05:21.016815Z","iopub.status.idle":"2024-04-18T02:05:21.379111Z","shell.execute_reply.started":"2024-04-18T02:05:21.016788Z","shell.execute_reply":"2024-04-18T02:05:21.378161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"# Average meter to keep track of metrics/loss during training\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        # Average is simply the sum divided by the count\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.380580Z","iopub.execute_input":"2024-04-18T02:05:21.381234Z","iopub.status.idle":"2024-04-18T02:05:21.388836Z","shell.execute_reply.started":"2024-04-18T02:05:21.381196Z","shell.execute_reply":"2024-04-18T02:05:21.387754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average meter to keep track of metrics/loss during training\nclass R2_METRIC(object):\n    def __init__(self):\n        self.reset()\n        self.y_mean = torch.tensor(train0[CONFIG.TARGET_COLUMNS].median(axis=0).values).to('cuda')\n\n    def reset(self):\n        self.avg = torch.zeros(CONFIG.N_TARGETS).to('cuda')\n        self.rss = torch.zeros(CONFIG.N_TARGETS).to('cuda')\n        self.tss = torch.zeros(CONFIG.N_TARGETS).to('cuda')\n\n    def update(self, y_pred, y_true, mean=False):\n        self.rss += torch.sum((y_true - y_pred)**2, dim=0)\n        self.tss += torch.sum((y_true - self.y_mean)**2, dim=0)\n        self.avg = 1 - (self.rss / torch.maximum(self.tss, CONFIG.EPS_CUDA))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.389972Z","iopub.execute_input":"2024-04-18T02:05:21.390255Z","iopub.status.idle":"2024-04-18T02:05:21.404499Z","shell.execute_reply.started":"2024-04-18T02:05:21.390231Z","shell.execute_reply":"2024-04-18T02:05:21.403609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"# Y_SHIFT As Torch Tensor On GPU\nY_SHIFT_CUDA = torch.tensor(Y_SHIFT).to('cuda')\nY_STD_CUDA = torch.tensor(Y_STD).to('cuda')\n# Is Log Feature Flag\nIS_LOG_FEATURE = torch.tensor(np.isin(CONFIG.TARGET_COLUMNS, LOG_FEATURES)).to('cuda')\n\ndef denormalize(y_pred, y_true=None):\n    # Scale Back\n    y_pred = (y_pred * Y_STD_CUDA) + Y_SHIFT_CUDA\n    # Log Scale\n    y_pred = torch.where(IS_LOG_FEATURE, 10**y_pred, y_pred)\n    # Optionally Denormalize y_true\n    if y_true is not None:\n        y_true = (y_true * Y_STD_CUDA) + Y_SHIFT_CUDA\n        y_true = torch.where(IS_LOG_FEATURE, 10**y_true, y_true)\n        return y_pred, y_true\n    else:\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.405778Z","iopub.execute_input":"2024-04-18T02:05:21.406087Z","iopub.status.idle":"2024-04-18T02:05:21.417018Z","shell.execute_reply.started":"2024-04-18T02:05:21.406062Z","shell.execute_reply":"2024-04-18T02:05:21.416176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean feature values used to compute R2 loss\nY_MEDIAN = torch.tensor(CONFIG.TARGET_MEDIANS).to('cuda')\n# Total Variation\nMEAN_VARIATION = torch.tensor(\n        (CONFIG.TARGET_MEDIANS - y_train_mask_raw)\n    ).abs().mean(dim=0).to('cuda')\n# R2 Loss\ndef r2_loss_fn(y_pred, y_true):\n    B = len(y_pred)\n    # Compute column wise sum of residuals and totals\n    ss_res = (y_true - y_pred)**2\n    ss_total = (y_true - Y_MEDIAN)**2\n    # r2 ranging from 0 to infinity\n    loss = torch.sum(ss_res, dim=0) / torch.maximum(torch.sum(ss_total, dim=0), CONFIG.EPS_CUDA)\n    # Return Mean Of Loss\n    return torch.mean(loss)\n\nr2_loss_fn(denormalize(outputs['label']), denormalize(y_batch.to('cuda')))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.418547Z","iopub.execute_input":"2024-04-18T02:05:21.419292Z","iopub.status.idle":"2024-04-18T02:05:21.443649Z","shell.execute_reply.started":"2024-04-18T02:05:21.419237Z","shell.execute_reply":"2024-04-18T02:05:21.442729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_step():\n    # Loss Function\n    R2_LOSS_FN = r2_loss_fn\n    # Put model in evaluation mode\n    model.eval()\n    # Metrics Trackers\n    R2 = R2_METRIC()\n    R2_LOSS = AverageMeter()\n    # Iterave Over Validation Set\n    for step, (X_sample, y_true) in enumerate(val_dataloader):\n        y_true = y_true.to('cuda')\n        # Put label on GPU\n        with torch.no_grad():\n            for k, v in X_sample.items():\n                X_sample[k] = v.to('cuda')\n            # Forward Pass\n            y_pred = model(X_sample)['label']\n        # Denormalize\n        y_pred_raw, y_true_raw = denormalize(y_pred, y_true)\n        # Loss\n        r2_loss = R2_LOSS_FN(y_pred_raw, y_true_raw)\n        # Update Loss Metrics\n        R2_LOSS.update(r2_loss)\n        # Update Metrics\n        R2.update(y_pred_raw, y_true_raw)\n        # Logs\n        r2_str = \", \".join(\n            [f\"{f}: {v:+.3f}\" for f, v in zip(CONFIG.TARGET_COLUMNS_TEST, R2.avg)\n        ])\n        if not CONFIG.IS_INTERACTIVE and (step + 1) == CONFIG.N_VAL_STEPS_PER_EPOCH:\n            print(\n                f'VAL | R2 loss: {R2_LOSS.avg:.4f}, R2: {R2.avg.mean():.3f}, {r2_str}' + (' ' * 10)\n            )\n        elif CONFIG.IS_INTERACTIVE:\n            print(\n                f'\\rVAL {step+1:02d}/{CONFIG.N_VAL_STEPS_PER_EPOCH} | R2 loss: {R2_LOSS.avg:.4f}, ' +\n                f'R2: {R2.avg.mean():.3f}, {r2_str}' + (' ' * 10),\n                end='\\n' if (step + 1) == CONFIG.N_VAL_STEPS_PER_EPOCH else '', flush=True,\n            )\n    \nvalidation_step()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:05:21.445230Z","iopub.execute_input":"2024-04-18T02:05:21.445566Z","iopub.status.idle":"2024-04-18T02:07:01.743459Z","shell.execute_reply.started":"2024-04-18T02:05:21.445541Z","shell.execute_reply":"2024-04-18T02:07:01.742339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Loss\nR2_LOSS_FN = r2_loss_fn\n# Optimizer\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=CONFIG.LR_MAX,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n# Learning Rate Scheduler\nLR_SCHEDULER = get_lr_scheduler(optimizer)\n# Metrics Trackers\nR2 = R2_METRIC()\nR2_LOSS = AverageMeter()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:07:01.744725Z","iopub.execute_input":"2024-04-18T02:07:01.745019Z","iopub.status.idle":"2024-04-18T02:07:01.764051Z","shell.execute_reply.started":"2024-04-18T02:07:01.744995Z","shell.execute_reply":"2024-04-18T02:07:01.763061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.TRAIN_MODEL:\n    for epoch in range(CONFIG.N_EPOCHS):\n        # Reset Metrics\n        R2.reset()\n        R2_LOSS.reset()\n        # Put model in training mode\n        model.train()\n        # Iterate Over Training Dataloader\n        for step, (X_batch, y_true) in enumerate(train_dataloader):\n            # Put batch on GPU\n            for k, v in X_batch.items():\n                X_batch[k] = v.to('cuda')\n            y_true = y_true.to('cuda')\n            # Step Time\n            t_start = time.perf_counter_ns()\n            # Forward Pass\n            y_pred = model(X_batch)['label']\n            # Denormalize\n            y_pred_raw, y_true_raw = denormalize(y_pred, y_true)\n            # Loss\n            r2_loss = R2_LOSS_FN(y_pred_raw, y_true_raw)\n            # Update Loss Metrics\n            R2_LOSS.update(r2_loss)\n            # Compute Gradients\n            r2_loss.backward()\n            # Backward Pass\n            optimizer.step()\n            # Zero Out Gradients\n            optimizer.zero_grad()\n            # Update Metrics\n            R2.update(y_pred_raw, y_true_raw)\n            # Compute R2 Metrics String\n            r2_str = \", \".join([\n                f\"{f}: {v:+.3f}\" for f, v in zip(CONFIG.TARGET_COLUMNS_TEST, R2.avg)\n            ])\n            # Logs\n            if not CONFIG.IS_INTERACTIVE and (step + 1) == CONFIG.N_STEPS_PER_EPOCH:\n                print(\n                    f'EPOCH {epoch+1:02d} {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'R2 loss: {R2_LOSS.avg:.4f}, R2: {R2.avg.mean():+.3f}, {r2_str}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                )\n            elif CONFIG.IS_INTERACTIVE:\n                print(\n                    f'\\rEPOCH {epoch+1:02d} {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'R2 loss: {R2_LOSS.avg:.4f}, R2: {R2.avg.mean():+.3f}, {r2_str}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n                )\n            # Learning Rate Scheduler Step\n            LR_SCHEDULER.step()\n        # Validation Step\n        validation_step()\n\n# Save entire model object\ntorch.save(model, 'if_log_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:07:01.765362Z","iopub.execute_input":"2024-04-18T02:07:01.765676Z","iopub.status.idle":"2024-04-18T04:03:30.299133Z","shell.execute_reply.started":"2024-04-18T02:07:01.765649Z","shell.execute_reply":"2024-04-18T04:03:30.298043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Minimum And Maximum Values To Clip Predictions\nTARGET_MIN = train0[CONFIG.TARGET_COLUMNS].values.min(axis=0)\nTARGET_MAX = train0[CONFIG.TARGET_COLUMNS].values.max(axis=0)\n# Submission Rows\nSUBMISSION_ROWS = []\n# Put Model in Evaluation Mode\nmodel.eval()\nfor i, (X_sample_test, test_id) in enumerate(tqdm(test_dataset)):\n    # Only 100 predictions in interactive mode\n    if CONFIG.IS_INTERACTIVE and i == 100:\n        break\n    # Put sample on GPU and add batch dimension\n    for k, v in X_sample_test.items():\n        X_sample_test[k] = v.to('cuda').unsqueeze(0)\n    # Prediction without gradients\n    with torch.no_grad():\n        y_pred = model(X_sample_test)['label']\n    # Reverse Scaling\n    y_pred, _ = denormalize(y_pred, y_pred)\n    y_pred = y_pred.detach().cpu().numpy().squeeze()\n    # Clip Values\n    y_pred = np.clip(y_pred, TARGET_MIN, TARGET_MAX)\n    # Add To Rows\n    row = { 'id': test_id }\n    # Add Predictions column by column\n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        # Remove \"_mean\" part of target column\n        row[k.replace('_mean', '')] = v\n    # Add To Submission Rows\n    SUBMISSION_ROWS.append(row)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make Submission CSV\nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\n\ndisplay(submission_df.head(30))\n\n# Make\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}